---
layout: post
title: Cost Performance Model (to evaluate MySQL on ARM)
img: ./images/blog11/cpm.png
author: Krunal Bauskar
update: 16th Sep 2020
---

ARM processors are fast gaining popularity in the High Performance Computing (HPC) space with multiple cloud providers providing powerful and flexible variants of ARM instances to boot. Users are still in a dilemma about whether running MySQL on ARM is really effective? To help ease this out we introduce a Cost-Performance-Model (#cpm). Model is generic in nature to help normalize computing configuration based on cost and could be used for other HPC kinds of software too.

## <span style="color:#4885ed">USP of ARM</span>

ARM is all about a lot of cores (may be less powerful compared to x86) running with lesser power there-by effectively generating cost savings. Let’s understand this fact with some data-points.

<img src="/images/blog11/img1.png" width="120%" class="centerimg"/>
<br>

Above table shows that booting ARM resources on any cloud-provider is cost effective (compared to x86).

Key is to find out if the said saving could be realized when running software. This sounds easy, just run the same version of software with the same configuration on the comparable machines and if both variants produce on-par performance then we can simply claim ARM is cost-effective compared to x86 since we are getting the same performance with reduced cost.

Challenges with the said model
 * Frequency differences (including turbo-boost mode)
 * CPU generation difference
 * x86 physical vs hyper-threaded cores
 * NUMA arrangement differences
 * Memory differences (speed and capacity (with large variants))
 * Scheduler differences (kernel-5.x has improved kernel for large number of cores)
 * Finding a comparable configuration (especially with bare-metals).

## <span style="color:#4885ed">Cost Performance Model (#cpm)</span>

With all those challenges let’s try to see if we can find a model that would make this comparison easy and effective.

**Core Idea:** Keeping cost constant can we get more throughput (there by more tps/per USD) from ARM based instance[s]?

In other words, if we have x USD to spend and we can get

**Option-1:** M ARM resources (read cores) <br>
**Option-2:** N x86 resources (read cores)
<br>
and if these M ARM resources can produce throughput > N x86 resources then it can be still termed as cost saving.
<br><br>

* Define in more formal word <br>
<span style="color:#de5246">**Given the on-par cost of both the resources can we exploit the ARM variant to its fullest benefit and get better performance when compared to x86**</span>
<hr>

Applying CPM to cloud instance from HuaweiCloud we can get

For around 3500 USD (per year) we can either get
* x86: 12 vCPU/48 GB => 3432 USD (yearly)
* ARM: 24 vCPU/48GB => 3578 USD (yearly)

<span style="color: #1aa260">**that is 2x more cores (In terms of computing power it is 12*3= 36 Ghz vs 24 * 2.6 = 62.4 Ghz).**</span>

Difference is even wider with bare metal servers. It is recommended to check with the respective cloud provider (the configuration I have access to the difference with BMS ranges from 2-5x).

In the upcoming blog series we will apply CPM to evaluate MySQL and its variants.

## <span style="color:#4885ed">Conclusion</span>

With cost normalized (using CPM) and keeping all other configurations the same it would be interesting to see if MySQL can use the extra computing power and produce more throughput. This will also help answer if running MySQL on ARM is really cost effective.

<br>
<em>If you have more questions/queries do let me know. Will try to answer them.</em>
